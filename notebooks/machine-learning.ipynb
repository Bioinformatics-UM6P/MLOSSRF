{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":224869804,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport gc\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.spatial.distance import cosine, mahalanobis\nfrom scipy.stats import ks_2samp, skew, kurtosis, entropy, f_oneway, norm\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Multiply, Add, BatchNormalization, LayerNormalization, MultiHeadAttention, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import plot_model\n\nimport shap\nfrom sklearn.inspection import permutation_importance\n\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"architectures\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\nos.makedirs(\"feature_analysis\", exist_ok=True)\nos.makedirs(\"optimization\", exist_ok=True)\n\ngc.enable()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-20T13:37:21.463139Z","iopub.status.idle":"2025-04-20T13:37:21.463451Z","shell.execute_reply.started":"2025-04-20T13:37:21.463270Z","shell.execute_reply":"2025-04-20T13:37:21.463281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntf.random.set_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.464822Z","iopub.status.idle":"2025-04-20T13:37:21.465113Z","shell.execute_reply.started":"2025-04-20T13:37:21.464966Z","shell.execute_reply":"2025-04-20T13:37:21.464978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/al-moutmir/data.csv')","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:37:21.466506Z","iopub.status.idle":"2025-04-20T13:37:21.466862Z","shell.execute_reply.started":"2025-04-20T13:37:21.466691Z","shell.execute_reply":"2025-04-20T13:37:21.466704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sampling Methods","metadata":{}},{"cell_type":"markdown","source":"## Train-Test Split","metadata":{}},{"cell_type":"code","source":"# Define feature matrix (X) and target variable (y)\nX = data.drop(columns=['grain_yield_kg', 'yield_quartile'])\ny = data['grain_yield_kg']\n\n# Random and Temporal Splits\nX_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n)\n\ntrain_data = data[data['growth_season'] != data['growth_season'].max()]\ntest_data = data[data['growth_season'] == data['growth_season'].max()]\n\nX_train_temp = train_data.drop(columns=['grain_yield_kg', 'yield_quartile'])\ny_train_temp = train_data['grain_yield_kg']\nX_test_temp = test_data.drop(columns=['grain_yield_kg', 'yield_quartile'])\ny_test_temp = test_data['grain_yield_kg']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.468704Z","iopub.status.idle":"2025-04-20T13:37:21.468967Z","shell.execute_reply.started":"2025-04-20T13:37:21.468856Z","shell.execute_reply":"2025-04-20T13:37:21.468868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Splits Evaluation\n### Distribution of Crops Across Splits","metadata":{}},{"cell_type":"code","source":"quartile_distribution = pd.concat([\n    data.loc[X_train_rand.index.intersection(data.index), 'crop']\n        .value_counts(normalize=True)\n        .rename(\"Random Train\"),\n    data.loc[X_test_rand.index.intersection(data.index), 'crop']\n        .value_counts(normalize=True)\n        .rename(\"Random Test\"),\n    data.loc[X_train_temp.index.intersection(data.index), 'crop']\n        .value_counts(normalize=True)\n        .rename(\"Temporal Train\"),\n    data.loc[X_test_temp.index.intersection(data.index), 'crop']\n        .value_counts(normalize=True)\n        .rename(\"Temporal Test\")\n], axis=1).fillna(0)\n\n# Define crop mapping\ncrop_mapping = {0: \"Barley\", 1: \"Dry Wheat\", 2: \"Soft Wheat\"}\n\n# Map crop labels to their names\nquartile_distribution.index = quartile_distribution.index.map(crop_mapping)\n\n# Plot the distribution\nquartile_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title(\"Distribution of Crops Across Splits\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Crops\")\nplt.legend(title=\"Splits\", loc=\"upper left\", bbox_to_anchor=(1, 1))\nplt.tight_layout()\n\n# Save the plot\nplt.savefig(os.path.join(\"feature_analysis\", \"target_distribution_across_splits.png\"), dpi=300)\n\n# Display the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.471411Z","iopub.status.idle":"2025-04-20T13:37:21.471689Z","shell.execute_reply.started":"2025-04-20T13:37:21.471538Z","shell.execute_reply":"2025-04-20T13:37:21.471581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cosine Similarity\nrandom_similarity = 1 - cosine(quartile_distribution[\"Random Train\"], quartile_distribution[\"Random Test\"])\ntemporal_similarity = 1 - cosine(quartile_distribution[\"Temporal Train\"], quartile_distribution[\"Temporal Test\"])\n\n# KL Divergence\nkl_random = entropy(quartile_distribution[\"Random Train\"], quartile_distribution[\"Random Test\"])\nkl_temporal = entropy(quartile_distribution[\"Temporal Train\"], quartile_distribution[\"Temporal Test\"])\n\n# Shannon Entropy\nentropy_random_train = entropy(quartile_distribution[\"Random Train\"])\nentropy_random_test = entropy(quartile_distribution[\"Random Test\"])\nentropy_temporal_train = entropy(quartile_distribution[\"Temporal Train\"])\nentropy_temporal_test = entropy(quartile_distribution[\"Temporal Test\"])\n\n# Output\nprint(f\"Cosine Similarity (Random Split): {random_similarity:.4f}\")\nprint(f\"Cosine Similarity (Temporal Split): {temporal_similarity:.4f}\")\nprint(f\"KL Divergence (Random Split): {kl_random:.4f}\")\nprint(f\"KL Divergence (Temporal Split): {kl_temporal:.4f}\")\nprint(f\"Shannon Entropy (Random Train): {entropy_random_train:.4f}\")\nprint(f\"Shannon Entropy (Temporal Train): {entropy_temporal_train:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.473048Z","iopub.status.idle":"2025-04-20T13:37:21.473809Z","shell.execute_reply.started":"2025-04-20T13:37:21.473661Z","shell.execute_reply":"2025-04-20T13:37:21.473680Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Distribution of Target Variable","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.kdeplot(y_train_rand, label=\"Random Train\", color=\"blue\", fill=True, alpha=0.3)\nsns.kdeplot(y_test_rand, label=\"Random Test\", color=\"red\", fill=True, alpha=0.3)\nsns.kdeplot(y_train_temp, label=\"Temporal Train\", color=\"green\", fill=True, alpha=0.3)\nsns.kdeplot(y_test_temp, label=\"Temporal Test\", color=\"orange\", fill=True, alpha=0.3)\nplt.title(\"Distribution of Target Variable (Yield)\")\nplt.xlabel(\"Yield (kg)\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(\"feature_analysis\", \"/target_distribution_comparison.png\"), dpi=300)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.474442Z","iopub.status.idle":"2025-04-20T13:37:21.474705Z","shell.execute_reply.started":"2025-04-20T13:37:21.474593Z","shell.execute_reply":"2025-04-20T13:37:21.474605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kolmogorov-Smirnov Test\nks_rand = ks_2samp(y_train_rand, y_test_rand)\nks_temp = ks_2samp(y_train_temp, y_test_temp)\n\n# Descriptive Statistics\ndef describe_data(y):\n    return {\n        \"Mean\": np.round(y.mean(), 4),\n        \"Median\": np.round(y.median(), 4),\n        \"Variance\": np.round(y.var(), 4),\n        \"Skewness\": np.round(skew(y), 4),\n        \"Kurtosis\": np.round(kurtosis(y), 4)\n    }\n\nstats_rand_train = describe_data(y_train_rand)\nstats_rand_test = describe_data(y_test_rand)\nstats_temp_train = describe_data(y_train_temp)\nstats_temp_test = describe_data(y_test_temp)\n\n# Output\nprint(f\"KS Test (Random Split): Statistic={ks_rand.statistic:.4f}, p-value={ks_rand.pvalue:.4f}\")\nprint(f\"KS Test (Temporal Split): Statistic={ks_temp.statistic:.4f}, p-value={ks_temp.pvalue:.4f}\")\nprint(\"Descriptive Statistics (Random Train):\", stats_rand_train)\nprint(\"Descriptive Statistics (Temporal Train):\", stats_temp_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.475907Z","iopub.status.idle":"2025-04-20T13:37:21.476200Z","shell.execute_reply.started":"2025-04-20T13:37:21.476069Z","shell.execute_reply":"2025-04-20T13:37:21.476085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning Models","metadata":{}},{"cell_type":"code","source":"# Function to calculate core metrics\ndef calculate_metrics(y_true, y_pred, n_features=None):\n    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n\n    r2 = r2_score(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    return {\n        \"R2\": r2,\n        \"MAE\": mae,\n        \"MAPE\": mape,\n        \"RMSE\": rmse,\n    }\n\n# Function to calculate grouped metrics\ndef calculate_grouped_metrics(df, group_col, y_true, y_pred):\n    grouped = df.groupby(group_col).apply(\n        lambda group: calculate_metrics(group[y_true], group[y_pred])\n    )\n    return grouped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.479346Z","iopub.status.idle":"2025-04-20T13:37:21.479665Z","shell.execute_reply.started":"2025-04-20T13:37:21.479465Z","shell.execute_reply":"2025-04-20T13:37:21.479489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ML Models\nmodels = {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'XGBoost': XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42),\n    'LightGBM': LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42),\n    'AdaBoost': AdaBoostRegressor(\n        base_estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n        n_estimators=10,\n        learning_rate=0.05,\n        random_state=42\n    ),\n    'CatBoost': CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42),\n    'GradientBoosting': GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05, random_state=42),\n    'Ridge': Ridge(alpha=1.0, random_state=42),\n    'Lasso': Lasso(alpha=0.1, random_state=42),\n    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n    'Stacking_LR': StackingRegressor(\n        estimators=[\n            ('cb', CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)),\n            ('xgb', XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)),\n            ('lgbm', LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42))\n        ],\n        final_estimator=LinearRegression()\n    ),\n    'Stacking_RandomForestRegressor': StackingRegressor(\n        estimators=[\n            ('cb', CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)),\n            ('xgb', XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)),\n            ('lgbm', LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42))\n        ],\n        final_estimator=RandomForestRegressor(n_estimators=100, random_state=42)\n    ),\n    'Stacking_CatBoostRegressor': StackingRegressor(\n        estimators=[\n            ('cb', CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)),\n            ('xgb', XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)),\n            ('lgbm', LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42))\n        ],\n        final_estimator=CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)\n    ),\n    'Stacking_LGBMRegressor': StackingRegressor(\n        estimators=[\n            ('cb', CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)),\n            ('xgb', XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)),\n            ('lgbm', LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42))\n        ],\n        final_estimator=LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42)\n    ),\n    'Stacking_XGBRegressor': StackingRegressor(\n        estimators=[\n            ('cb', CatBoostRegressor(iterations=2000, learning_rate=0.05, verbose=0, random_seed=42)),\n            ('xgb', XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)),\n            ('lgbm', LGBMRegressor(n_estimators=2000, learning_rate=0.05, verbose=-1, random_state=42))\n        ],\n        final_estimator=XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state=42)\n    )\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.480812Z","iopub.status.idle":"2025-04-20T13:37:21.481069Z","shell.execute_reply.started":"2025-04-20T13:37:21.480953Z","shell.execute_reply":"2025-04-20T13:37:21.480965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate each model\nresults = {}\ngrouped_results = {}\ny_pred_ensemble_rand = []\ny_pred_ensemble_temp = []\n\nfor model_name, model in models.items():\n    print(f\"\\n Evaluate {model_name}\")\n\n    # Random Split\n    model.fit(X_train_rand, y_train_rand, sample_weight=sample_weights_rand)\n    y_pred_rand = model.predict(X_test_rand)\n    rand_metrics = calculate_metrics(y_test_rand, y_pred_rand)\n    rand_df = X_test_rand.copy()\n    rand_df['y_true'], rand_df['y_pred'] = y_test_rand, y_pred_rand\n    \n    # Save model 1\n    joblib.dump(model, f\"models/{model_name}_random_split.pkl\")\n\n    # Temporal Split\n    model.fit(X_train_temp, y_train_temp)\n    y_pred_temp = model.predict(X_test_temp)\n    temp_metrics = calculate_metrics(y_test_temp, y_pred_temp)\n    temp_df = X_test_temp.copy()\n    temp_df['y_true'], temp_df['y_pred'] = y_test_temp, y_pred_temp\n\n    # Save model 2\n    joblib.dump(model, f\"models/{model_name}_temporal_split.pkl\")\n\n    # Grouped Metrics\n    grouped_rand_crop = calculate_grouped_metrics(rand_df, 'crop', 'y_true', 'y_pred')\n    grouped_temp_crop = calculate_grouped_metrics(temp_df, 'crop', 'y_true', 'y_pred')\n\n    # Store Results\n    results[model_name] = {'Random Split': rand_metrics, 'Temporal Split': temp_metrics}\n    grouped_results[model_name] = {\n        'Random Split Crop': grouped_rand_crop,\n        'Temporal Split Crop': grouped_temp_crop,\n    }\n\n    # For ensemble predictions\n    y_pred_ensemble_rand.append(y_pred_rand)\n    y_pred_ensemble_temp.append(y_pred_temp)\n\n# Flatten the results into a DataFrame\ndef flatten_results(results):\n    flattened = []\n    for model, splits in results.items():\n        for split, metrics in splits.items():\n            if isinstance(metrics, dict):\n                metrics['Model'] = model\n                metrics['Split'] = split\n                flattened.append(metrics)\n    return pd.DataFrame(flattened)\n\n# Flatten the results\nresults_df = flatten_results(results)\n\n# Pivot the table for better readability\nresults_pivot = results_df.pivot(index='Model', columns='Split',\n                                 values=['R2', 'MAE', 'MAPE', 'RMSE'])\n# Clean up the column names\nresults_pivot.columns = [f\"{metric}_{split}\" for metric, split in results_pivot.columns]\nresults_pivot.reset_index(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.482890Z","iopub.status.idle":"2025-04-20T13:37:21.483172Z","shell.execute_reply.started":"2025-04-20T13:37:21.483029Z","shell.execute_reply":"2025-04-20T13:37:21.483041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network Models","metadata":{}},{"cell_type":"code","source":"# Prepare data\nscaler = MinMaxScaler()\n\nX_train_rand_scaled = scaler.fit_transform(X_train_rand)\nX_test_rand_scaled = scaler.transform(X_test_rand)\nX_train_temp_scaled = scaler.fit_transform(X_train_temp)\nX_test_temp_scaled = scaler.transform(X_test_temp)\n\ninput_dim = X_train_rand_scaled.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.484278Z","iopub.status.idle":"2025-04-20T13:37:21.484762Z","shell.execute_reply.started":"2025-04-20T13:37:21.484586Z","shell.execute_reply":"2025-04-20T13:37:21.484602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Neural Network Architectures\ndef dnn(input_dim):\n    model = Sequential([\n        Dense(1024, activation='relu', input_dim=input_dim),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(128, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.2),\n        Dense(1, activation='linear')\n    ])\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n    return model\n\ndef feature_attention_nn(input_dim):\n    inputs = Input(shape=(input_dim,))\n    attention_weights = Dense(input_dim, activation='softmax', name=\"Attention_Weights\")(inputs)\n    weighted_inputs = Multiply()([inputs, attention_weights])\n\n    x = Dense(1024, activation='relu')(weighted_inputs)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    outputs = Dense(1, activation='linear')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n    return model\n\ndef resnet(input_dim):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(512, activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n\n    shortcut = Dense(256, activation='linear')(inputs)\n    x = Add()([x, shortcut])\n\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    outputs = Dense(1, activation='linear')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n    return model\n\ndef transformer(input_dim):\n    # Input layer\n    inputs = Input(shape=(input_dim,))\n\n    # Expand dimensions to create a sequence-like structure\n    reshaped_inputs = Reshape((1, input_dim))(inputs)  # Sequence length = 1, feature dim = input_dim\n\n    # Multi-Head Attention\n    attention_output = MultiHeadAttention(num_heads=4, key_dim=input_dim // 4)(reshaped_inputs, reshaped_inputs)\n    attention_output = LayerNormalization()(attention_output + reshaped_inputs)  # Skip connection\n\n    # Feedforward Network\n    ffn = Dense(input_dim, activation='relu')(attention_output)  # Ensure matching dimensions\n    ffn = Dense(input_dim, activation='relu')(ffn)  # Match dimensions with attention_output\n    ffn = LayerNormalization()(ffn + attention_output)  # Skip connection\n\n    # Remove sequence dimension for final output\n    flattened_ffn = Reshape((input_dim,))(ffn)  # Flatten to (None, input_dim)\n\n    # Output layer\n    outputs = Dense(1, activation='linear')(flattened_ffn)\n\n    # Compile the model\n    model = Model(inputs=inputs, outputs=outputs, name=\"Transformer_Model\")\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n\n    return model\n\ndef autoencoder_regressor(input_dim):\n    inputs = Input(shape=(input_dim,))\n    encoded = Dense(256, activation='relu')(inputs)\n    encoded = BatchNormalization()(encoded)\n    encoded = Dense(128, activation='relu')(encoded)\n    encoded = BatchNormalization()(encoded)\n\n    decoded = Dense(256, activation='relu')(encoded)\n    decoded = BatchNormalization()(decoded)\n    decoded = Dense(input_dim, activation='linear')(decoded)\n\n    regression_output = Dense(1, activation='linear')(encoded)\n\n    model = Model(inputs=inputs, outputs=regression_output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n    return model\n\n# Train, evaluate, and save models for both splits\nresults_nn = []\n\n# Define model functions and names\nmodel_functions = [dnn, feature_attention_nn, resnet, transformer]\nmodel_names = ['DNN', 'FeatureAttentionNN', 'ResNet', 'Transformer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.485831Z","iopub.status.idle":"2025-04-20T13:37:21.486146Z","shell.execute_reply.started":"2025-04-20T13:37:21.486032Z","shell.execute_reply":"2025-04-20T13:37:21.486044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save architectures as images for visualization\narchitecture_dir = \"architectures\"\n\n# Define the models and their corresponding functions\nmodel_functions = [dnn, feature_attention_nn, resnet, transformer]\nmodel_names = ['DNN', 'FeatureAttentionNN', 'ResNet', 'Transformer']\n\n# Save and visualize model architectures\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\nfor idx, (model_fn, model_name) in enumerate(zip(model_functions, model_names)):\n    # Initialize the model\n    model = model_fn(input_dim)\n    \n    # Save the model architecture\n    architecture_path = f\"{architecture_dir}/{model_name}_architecture.png\"\n    plot_model(model, to_file=architecture_path, show_shapes=True, show_layer_names=True)\n\n    # Display the model architecture\n    img = plt.imread(architecture_path)\n    axs[idx].imshow(img)\n    axs[idx].axis('off')\n    axs[idx].set_title(model_name)\n\n# Display all architectures in a single row\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.486910Z","iopub.status.idle":"2025-04-20T13:37:21.487151Z","shell.execute_reply.started":"2025-04-20T13:37:21.487040Z","shell.execute_reply":"2025-04-20T13:37:21.487050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through models and splits\nfor split_name, (X_train_scaled, y_train, X_test_scaled, y_test) in zip(\n    [\"Random Split\", \"Temporal Split\"],\n    [\n        (X_train_rand_scaled, y_train_rand, X_test_rand_scaled, y_test_rand),\n        (X_train_temp_scaled, y_train_temp, X_test_temp_scaled, y_test_temp),\n    ],\n):\n    for model_fn, model_name in zip(model_functions, model_names):\n        print(f\"\\n Evaluate {model_name} - {split_name}\")\n        # Initialize and train the model\n        model = model_fn(input_dim)\n        early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n\n        model.fit(\n            X_train_scaled, y_train,\n            validation_data=(X_test_scaled, y_test),\n            epochs=2000, batch_size=32, verbose=0,\n            callbacks=[early_stopping, reduce_lr]\n        )\n\n        # Predict and calculate metrics\n        y_pred = model.predict(X_test_scaled).flatten()\n        metrics = calculate_metrics(y_test, y_pred)\n\n        # Save results for both splits\n        results_nn.append({\n            'Model': model_name,\n            f\"R2_{split_name}\": metrics['R2'],\n            f\"MAE_{split_name}\": metrics['MAE'],\n            f\"MAPE_{split_name}\": metrics['MAPE'],\n            f\"RMSE_{split_name}\": metrics['RMSE']\n        })\n\n        # Grouped Metrics\n        grouped_rand_crop = calculate_grouped_metrics(rand_df, 'crop', 'y_true', 'y_pred')\n        grouped_temp_crop = calculate_grouped_metrics(temp_df, 'crop', 'y_true', 'y_pred')\n    \n        # Store Results\n        grouped_results[model_name] = {\n            'Random Split Crop': grouped_rand_crop,\n            'Temporal Split Crop': grouped_temp_crop,\n        }\n        \n\n        # Save the model\n        model.save(f\"models/{model_name}_{split_name.replace(' ', '_')}.keras\")\n\n# Combine results into a DataFrame\nresults_nn_combined = pd.DataFrame(results_nn)\n\n# Merge random and temporal split results into one row per model\nresults_nn_final = results_nn_combined.groupby('Model').agg('first').reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.488698Z","iopub.status.idle":"2025-04-20T13:37:21.489020Z","shell.execute_reply.started":"2025-04-20T13:37:21.488850Z","shell.execute_reply":"2025-04-20T13:37:21.488865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Appending the results_nn_final to results_pivot\ncombined_results = pd.concat([results_pivot, results_nn_final], ignore_index=True)\ncombined_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.489875Z","iopub.status.idle":"2025-04-20T13:37:21.490108Z","shell.execute_reply.started":"2025-04-20T13:37:21.489994Z","shell.execute_reply":"2025-04-20T13:37:21.490005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Selecting the top 3 models based on MAPE for Random and Temporal Splits\ntop_3_random_models = combined_results.nsmallest(3, \"MAPE_Random Split\")[\"Model\"].tolist()\ntop_3_temporal_models = combined_results.nsmallest(3, \"MAPE_Temporal Split\")[\"Model\"].tolist()\n\n# Function to plot and save prediction vs actual values\ndef plot_and_save(model_name, y_test, y_pred, split_name):\n    plt.figure(figsize=(7, 7))\n    plt.scatter(y_test, y_pred, alpha=0.7, label=\"Predicted vs Actual\")\n    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Perfect Prediction\")\n    plt.title(f\"Predicted vs Actual - {model_name} - {split_name}\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"results/Predicted vs Actual - {model_name} - {split_name}.png\")\n    plt.show()\n    plt.close()\n    \n# Function to extract predictions and ground truth\ndef get_predictions_and_actuals(model_name, split_type):\n    if split_type == \"Random Split\":\n        y_test = y_test_rand\n        y_pred = y_pred_ensemble_rand[top_3_random_models.index(model_name)]\n    elif split_type == \"Temporal Split\":\n        y_test = y_test_temp\n        y_pred = y_pred_ensemble_temp[top_3_temporal_models.index(model_name)]\n    return y_test, y_pred\n\n# Plotting and saving the predictions for the top models\nfor model_name in top_3_random_models:\n    y_test, y_pred = get_predictions_and_actuals(model_name, \"Random Split\")\n    plot_and_save(model_name, y_test, y_pred, \"Random Split\")\n\nfor model_name in top_3_temporal_models:\n    y_test, y_pred = get_predictions_and_actuals(model_name, \"Temporal Split\")\n    plot_and_save(model_name, y_test, y_pred, \"Temporal Split\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.491776Z","iopub.status.idle":"2025-04-20T13:37:21.491996Z","shell.execute_reply.started":"2025-04-20T13:37:21.491889Z","shell.execute_reply":"2025-04-20T13:37:21.491898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Crop-Specific Model Evaluation","metadata":{}},{"cell_type":"code","source":"wide_table_with_crops = []\n\n# Iterate through each model and its splits\nfor model, splits in grouped_results.items():\n    # Iterate through each split (e.g., 'Random Split Crop', 'Temporal Split Crop')\n    for split, data in splits.items():\n        # Iterate through each crop\n        for crop_index, metrics in enumerate(data):\n            # Initialize a row dictionary for the current crop\n            row = {'Model': model, 'Split': split, 'Crop': crop_index}\n            # Add metrics to the row\n            row.update(metrics)\n            # Append the row to the wide_table\n            wide_table_with_crops.append(row)\n\n# Convert the table list into a DataFrame\nwide_results_table_with_crops = pd.DataFrame(wide_table_with_crops)\n\n# Pivot the DataFrame to the desired wide format\nwide_results_table = wide_results_table_with_crops.pivot(\n    index=['Model', 'Crop'],\n    columns='Split',\n    values=['R2', 'MAE', 'MAPE', 'RMSE']\n)\n\n# Flatten the MultiIndex columns\nwide_results_table.columns = [\n    f\"{metric}_{split}\" for metric, split in wide_results_table.columns\n]\n\n# Reset index to make it a clean DataFrame\nwide_results_table.reset_index(inplace=True)\nwide_results_table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.493076Z","iopub.status.idle":"2025-04-20T13:37:21.493303Z","shell.execute_reply.started":"2025-04-20T13:37:21.493187Z","shell.execute_reply":"2025-04-20T13:37:21.493199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Learning","metadata":{}},{"cell_type":"code","source":"# Define helper function to load models\ndef load_model(filepath):\n    if filepath.endswith(\".pkl\"):\n        return joblib.load(filepath)  # For machine learning models\n    elif filepath.endswith(\".keras\"):\n        return keras.models.load_model(filepath)  # For neural network models\n    else:\n        raise ValueError(f\"Unsupported file format for {filepath}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.493964Z","iopub.status.idle":"2025-04-20T13:37:21.494170Z","shell.execute_reply.started":"2025-04-20T13:37:21.494073Z","shell.execute_reply":"2025-04-20T13:37:21.494082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Unified Ensemble Learning","metadata":{}},{"cell_type":"code","source":"# Build Ensemble of Top N Models\nensemble_sizes = list(range(2, 6))\nensemble_results = []\n\nfor n in ensemble_sizes:\n    # Select top N models for Random and Temporal splits based on MAE\n    sorted_models_rand = sorted(\n        {k: v for k, v in results.items() if 'Ensemble' not in k}.items(),\n        key=lambda x: x[1]['Random Split']['MAPE']\n    )[:n]\n    sorted_models_temp = sorted(\n        {k: v for k, v in results.items() if 'Ensemble' not in k}.items(),\n        key=lambda x: x[1]['Temporal Split']['MAPE']\n    )[:n]\n\n    top_model_names_rand = [m[0] for m in sorted_models_rand]\n    top_model_names_temp = [m[0] for m in sorted_models_temp]\n\n    # Compute weights for the ensemble (inverse of MAE)\n    weights_rand = np.array([1 / results[model]['Random Split']['MAPE'] for model in top_model_names_rand])\n    weights_temp = np.array([1 / results[model]['Temporal Split']['MAPE'] for model in top_model_names_temp])\n\n    # Normalize weights to sum to 1\n    weights_rand /= weights_rand.sum()\n    weights_temp /= weights_temp.sum()\n\n    # Compute ensemble predictions (weighted average)\n    y_pred_ensemble_rand = np.zeros_like(y_test_rand)\n    y_pred_ensemble_temp = np.zeros_like(y_test_temp)\n\n    for i, model_name in enumerate(top_model_names_rand):\n        model_path = f\"models/{model_name}_random_split\"\n        if os.path.exists(f\"{model_path}.pkl\"):\n            model = load_model(f\"{model_path}.pkl\")\n        elif os.path.exists(f\"{model_path}.keras\"):\n            model = load_model(f\"{model_path}.keras\")\n        else:\n            continue\n\n        y_pred_rand = model.predict(X_test_rand)\n        y_pred_ensemble_rand += weights_rand[i] * y_pred_rand\n\n    for i, model_name in enumerate(top_model_names_temp):\n        model_path = f\"models/{model_name}_temporal_split\"\n        if os.path.exists(f\"{model_path}.pkl\"):\n            model = load_model(f\"{model_path}.pkl\")\n        elif os.path.exists(f\"{model_path}.keras\"):\n            model = load_model(f\"{model_path}.keras\")\n        else:\n            continue\n\n        y_pred_temp = model.predict(X_test_temp)\n        y_pred_ensemble_temp += weights_temp[i] * y_pred_temp\n\n    # Calculate ensemble metrics\n    ensemble_rand_metrics = calculate_metrics(y_test_rand, y_pred_ensemble_rand)\n    ensemble_temp_metrics = calculate_metrics(y_test_temp, y_pred_ensemble_temp)\n\n    # Store results including model names\n    ensemble_results.append({\n        'Model': f\"Ensemble_{n}\",\n        'Number of Models': n,\n        'Model Names (Random Split)': ', '.join(top_model_names_rand),\n        'Model Names (Temporal Split)': ', '.join(top_model_names_temp),\n        **{f\"{metric}_Random Split\": value for metric, value in ensemble_rand_metrics.items()},\n        **{f\"{metric}_Temporal Split\": value for metric, value in ensemble_temp_metrics.items()}\n    })\n\n# Convert results into a DataFrame\nensemble_results_df = pd.DataFrame(ensemble_results)\nensemble_results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.495737Z","iopub.status.idle":"2025-04-20T13:37:21.496044Z","shell.execute_reply.started":"2025-04-20T13:37:21.495877Z","shell.execute_reply":"2025-04-20T13:37:21.495892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Diebold-Mariano Test of Ensemble Learning ","metadata":{}},{"cell_type":"code","source":"# Perform Diebold-Mariano Test for Top Ensembles for Random and Temporal Splits\ndm_test_results = []\n\n# Define DM Test function\ndef dm_test(y_true, y_pred1, y_pred2, h=1, loss=\"MSE\"):\n    \"\"\"\n    Perform Diebold-Mariano test.\n\n    Parameters:\n    - y_true: Array-like, true values.\n    - y_pred1: Array-like, predictions from Model 1.\n    - y_pred2: Array-like, predictions from Model 2.\n    - h: Int, forecast horizon (default=1).\n    - loss: String, loss function ('MSE' or 'MAE').\n\n    Returns:\n    - DM statistic and p-value.\n    \"\"\"\n    e1 = y_true - y_pred1\n    e2 = y_true - y_pred2\n    d = e1**2 - e2**2 if loss == \"MSE\" else np.abs(e1) - np.abs(e2)\n    d_mean = np.mean(d)\n    n = len(d)\n    gamma = np.zeros(h)\n    for lag in range(h):\n        gamma[lag] = np.sum((d[:-lag] - d_mean) * (d[lag:] - d_mean)) / (n - lag) if lag > 0 else np.var(d)\n    dm_stat = d_mean / np.sqrt((gamma[0] + 2 * np.sum(gamma[1:])) / n)\n    p_value = 2 * (1 - norm.cdf(np.abs(dm_stat)))\n    return dm_stat, p_value\n\n# Extract top #1 ensemble for Random and Temporal splits\ntop_random_ensemble = ensemble_results_df.iloc[0]  # First row corresponds to top ensemble for Random Split\ntop_temporal_ensemble = ensemble_results_df.iloc[0]  # First row corresponds to top ensemble for Temporal Split\n\n# Parse model names for each ensemble\ntop_random_models = top_random_ensemble['Model Names (Random Split)'].split(', ')\ntop_temporal_models = top_temporal_ensemble['Model Names (Temporal Split)'].split(', ')\n\n# Predictions dictionary for ensembles\nmodel_predictions = {\n    'Random Split': {},\n    'Temporal Split': {}\n}\n\n# Helper function to load predictions for all models in an ensemble\ndef get_ensemble_predictions(models, X_test, split_type):\n    predictions = {}\n    for model_name in models:\n        model_path = f\"models/{model_name}_{split_type.replace(' ', '_').lower()}\"\n        if os.path.exists(f\"{model_path}.pkl\"):\n            model = load_model(f\"{model_path}.pkl\")\n        elif os.path.exists(f\"{model_path}.keras\"):\n            model = load_model(f\"{model_path}.keras\")\n        else:\n            continue\n        predictions[model_name] = model.predict(X_test)\n    return predictions\n\n# Load predictions for Random and Temporal splits\nmodel_predictions['Random Split'] = get_ensemble_predictions(top_random_models, X_test_rand, 'random_split')\nmodel_predictions['Temporal Split'] = get_ensemble_predictions(top_temporal_models, X_test_temp, 'temporal_split')\n\n# DM Test for all model pairs within each split\nfor split_type, y_test, predictions in [(\"Random Split\", y_test_rand, model_predictions['Random Split']), \n                                        (\"Temporal Split\", y_test_temp, model_predictions['Temporal Split'])]:\n    model_names = list(predictions.keys())\n    for i, model_name_1 in enumerate(model_names):\n        for j, model_name_2 in enumerate(model_names):\n            if i >= j:  # Avoid duplicate comparisons and self-comparisons\n                continue\n\n            # Get predictions for the two models\n            y_pred1 = predictions[model_name_1]\n            y_pred2 = predictions[model_name_2]\n\n            # Calculate DM test statistics\n            dm_stat, p_value = dm_test(y_test, y_pred1, y_pred2, loss=\"MSE\")\n\n            # Store results\n            dm_test_results.append({\n                'Model 1': model_name_1,\n                'Model 2': model_name_2,\n                'Split': split_type,\n                'DM Statistic': dm_stat,\n                'P-Value': p_value\n            })\n\n# Convert results to DataFrame\ndm_test_results_df = pd.DataFrame(dm_test_results)\n\n# Display DM Test Results Table\ndm_test_results_df.sort_values(by=['Split', 'P-Value'], inplace=True)\ndm_test_results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.497321Z","iopub.status.idle":"2025-04-20T13:37:21.497675Z","shell.execute_reply.started":"2025-04-20T13:37:21.497468Z","shell.execute_reply":"2025-04-20T13:37:21.497504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Crop-Specific Ensemble Model Evaluation","metadata":{}},{"cell_type":"code","source":"ensemble_results_crops = []\n\nfor n in ensemble_sizes:\n    for crop in wide_results_table_with_crops['Crop'].unique():\n        crop_results = wide_results_table_with_crops[wide_results_table_with_crops['Crop'] == crop]\n\n        # Select top N models for Random and Temporal splits\n        sorted_models_rand = crop_results[crop_results['Split'] == 'Random Split Crop'].nsmallest(n, 'MAPE')\n        sorted_models_temp = crop_results[crop_results['Split'] == 'Temporal Split Crop'].nsmallest(n, 'MAPE')\n\n        top_model_names_rand = sorted_models_rand['Model'].tolist()\n        top_model_names_temp = sorted_models_temp['Model'].tolist()\n\n        weights_rand = np.array([1 / mae for mae in sorted_models_rand['MAPE']])\n        weights_temp = np.array([1 / mae for mae in sorted_models_temp['MAPE']])\n\n        weights_rand /= weights_rand.sum()\n        weights_temp /= weights_temp.sum()\n\n        X_test_rand_crop = X_test_rand[X_test_rand['crop'] == crop]\n        X_test_temp_crop = X_test_temp[X_test_temp['crop'] == crop]\n        y_test_rand_crop = y_test_rand[X_test_rand['crop'] == crop]\n        y_test_temp_crop = y_test_temp[X_test_temp['crop'] == crop]\n\n        y_pred_ensemble_rand = np.zeros_like(y_test_rand_crop, dtype=float)\n        y_pred_ensemble_temp = np.zeros_like(y_test_temp_crop, dtype=float)\n\n        # Aggregate predictions for Random Split\n        for i, model_name in enumerate(top_model_names_rand):\n            model_path = f\"models/{model_name}_random_split\"\n            if os.path.exists(f\"{model_path}.pkl\"):\n                model = load_model(f\"{model_path}.pkl\")\n            elif os.path.exists(f\"{model_path}.keras\"):\n                model = load_model(f\"{model_path}.keras\")\n            else:\n                continue\n\n            y_pred_rand = model.predict(X_test_rand_crop)\n            y_pred_ensemble_rand += weights_rand[i] * y_pred_rand\n\n        # Aggregate predictions for Temporal Split\n        for i, model_name in enumerate(top_model_names_temp):\n            model_path = f\"models/{model_name}_temporal_split\"\n            if os.path.exists(f\"{model_path}.pkl\"):\n                model = load_model(f\"{model_path}.pkl\")\n            elif os.path.exists(f\"{model_path}.keras\"):\n                model = load_model(f\"{model_path}.keras\")\n            else:\n                continue\n\n            y_pred_temp = model.predict(X_test_temp_crop)\n            y_pred_ensemble_temp += weights_temp[i] * y_pred_temp\n\n        # Calculate metrics for Random and Temporal Splits\n        ensemble_rand_metrics = calculate_metrics(y_test_rand_crop, y_pred_ensemble_rand)\n        ensemble_temp_metrics = calculate_metrics(y_test_temp_crop, y_pred_ensemble_temp)\n\n        # Append results\n        ensemble_results_crops.append({\n            'Model': f\"Ensemble_{n}\",\n            'Crop': crop,\n            'Number of Models': n,\n            'Model Names (Random Split)': ', '.join(top_model_names_rand),\n            'Model Names (Temporal Split)': ', '.join(top_model_names_temp),\n            **{f\"{metric}_Random Split Crop\": value for metric, value in ensemble_rand_metrics.items()},\n            **{f\"{metric}_Temporal Split Crop\": value for metric, value in ensemble_temp_metrics.items()}\n        })\n\n# Convert results to DataFrame\nensemble_results_crops_df = pd.DataFrame(ensemble_results_crops)\nensemble_results_crops_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.498642Z","iopub.status.idle":"2025-04-20T13:37:21.498913Z","shell.execute_reply.started":"2025-04-20T13:37:21.498786Z","shell.execute_reply":"2025-04-20T13:37:21.498799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Importance and Interpretability Analysis\n## Feature Analysis","metadata":{}},{"cell_type":"code","source":"# Set flag to include/exclude temporal models in analysis\ninclude_temporal = True  # Set to False to exclude temporal models\n\n# Filter top models (exclude ensemble models and stacking models) for Random Split\ntop_models_random = results_pivot[\n    ~results_pivot[\"Model\"].str.contains(\"Ensemble\") & ~results_pivot[\"Model\"].str.contains(\"Stacking\")\n]\ntop_models_random = top_models_random.nsmallest(3, \"MAPE_Random Split\")  # Top 3 for Random Split\n\n# Filter top models for Temporal Split if enabled\nif include_temporal:\n    top_models_temporal = results_pivot[\n        ~results_pivot[\"Model\"].str.contains(\"Ensemble\") & ~results_pivot[\"Model\"].str.contains(\"Stacking\")\n    ]\n    top_models_temporal = top_models_temporal.nsmallest(3, \"MAPE_Temporal Split\")  # Top 3 for Temporal Split\n\n# Combine the models while maintaining uniqueness\nunique_top_models = set(top_models_random[\"Model\"])\nif include_temporal:\n    unique_top_models.update(top_models_temporal[\"Model\"])\n\n# Placeholder for storing analysis results\nfeature_analysis_results = {}\n\n# Loop through unique models for analysis\nfor model_name in unique_top_models:\n    print(f\"\\nEvaluating {model_name}\")\n    model_path_random = f\"models/{model_name}_random_split.pkl\"\n    model_path_temporal = f\"models/{model_name}_temporal_split.pkl\"\n\n    # Initialize model variables\n    model_random = None\n    model_temporal = None\n\n    # Load models safely\n    try:\n        if os.path.exists(model_path_random):\n            model_random = joblib.load(model_path_random)\n        if include_temporal and os.path.exists(model_path_temporal):\n            model_temporal = joblib.load(model_path_temporal)\n    except Exception as e:\n        print(f\"Error loading model {model_name}: {e}\")\n        continue\n\n    try:\n        # Feature Importance\n        feature_importance_random = (\n            model_random.feature_importances_ if model_random and hasattr(model_random, \"feature_importances_\") else None\n        )\n        feature_importance_temporal = (\n            model_temporal.feature_importances_ if model_temporal and hasattr(model_temporal, \"feature_importances_\") else None\n        ) if include_temporal else None\n\n        # Permutation Importance\n        perm_importance_random = permutation_importance(\n            model_random, X_test_rand, y_test_rand, scoring='neg_mean_absolute_error'\n        ) if model_random else None\n        perm_importance_temporal = (\n            permutation_importance(model_temporal, X_test_temp, y_test_temp, scoring='neg_mean_absolute_error')\n            if include_temporal and model_temporal\n            else None\n        )\n\n        # SHAP Analysis\n        explainer_random = shap.TreeExplainer(model_random) if model_random else None\n        shap_values_random = explainer_random.shap_values(X_test_rand) if explainer_random else None\n\n        explainer_temporal = shap.TreeExplainer(model_temporal) if include_temporal and model_temporal else None\n        shap_values_temporal = explainer_temporal.shap_values(X_test_temp) if explainer_temporal else None\n\n        # Store results\n        feature_analysis_results[model_name] = {\n            \"feature_importance_random\": feature_importance_random,\n            \"feature_importance_temporal\": feature_importance_temporal,\n            \"perm_importance_random\": perm_importance_random.importances_mean if perm_importance_random else None,\n            \"perm_importance_temporal\": perm_importance_temporal.importances_mean if perm_importance_temporal else None,\n            \"shap_values_random\": shap_values_random,\n            \"shap_values_temporal\": shap_values_temporal,\n        }\n    except Exception as e:\n        print(f\"Error analyzing model {model_name}: {e}\")\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.499937Z","iopub.status.idle":"2025-04-20T13:37:21.500274Z","shell.execute_reply.started":"2025-04-20T13:37:21.500108Z","shell.execute_reply":"2025-04-20T13:37:21.500124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importance Plotting","metadata":{}},{"cell_type":"code","source":"import shap\n\n# Create a directory to save plots if it doesn't already exist\nfeature_analysis_dir = \"feature_analysis\"\nos.makedirs(feature_analysis_dir, exist_ok=True)\n\n# Define the number of top features to display\nTOP_FEATURES = 25\n\n# Loop through top models for visualization\nfor model_name, results in feature_analysis_results.items():\n    # RANDOM SPLIT ANALYSIS\n    if results.get(\"feature_importance_random\") is not None:\n        # Feature Importance Plot (Random Split)\n        sorted_idx_random = np.argsort(-results[\"feature_importance_random\"])[:TOP_FEATURES]\n        feature_names_random = X_train_rand.columns[sorted_idx_random]\n        fig, ax = plt.subplots(figsize=(12, 8))\n        sns.barplot(\n            x=results[\"feature_importance_random\"][sorted_idx_random],\n            y=feature_names_random,\n            ax=ax\n        )\n        ax.set_title(f\"Feature Importance - {model_name} - Random Split\")\n        ax.set_xlabel(\"Importance Score\")\n        ax.set_ylabel(\"Features\")\n        plt.tight_layout()\n        plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_feature_importance_random.png\")\n        plt.savefig(plot_path)\n        plt.show()\n        plt.close()\n\n    if results.get(\"perm_importance_random\") is not None:\n        # Permutation Importance Plot (Random Split)\n        perm_sorted_idx_random = np.argsort(-results[\"perm_importance_random\"])[:TOP_FEATURES]\n        feature_names_perm_random = X_train_rand.columns[perm_sorted_idx_random]\n        fig, ax = plt.subplots(figsize=(12, 8))\n        sns.barplot(\n            x=results[\"perm_importance_random\"][perm_sorted_idx_random],\n            y=feature_names_perm_random,\n            ax=ax\n        )\n        ax.set_title(f\"Permutation Importance - {model_name} - Random Split\")\n        ax.set_xlabel(\"Importance Score\")\n        ax.set_ylabel(\"Features\")\n        plt.tight_layout()\n        plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_permutation_importance_random.png\")\n        plt.savefig(plot_path)\n        plt.show()\n        plt.close()\n\n    if results.get(\"shap_values_random\") is not None:\n        # SHAP Summary Plot (Random Split)\n        fig, ax = plt.subplots(figsize=(12, 8))\n        shap.summary_plot(\n            results[\"shap_values_random\"], X_test_rand, max_display=TOP_FEATURES, show=False\n        )\n        plt.title(f\"SHAP Summary - {model_name} - Random Split\")\n        plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_shap_random.png\")\n        plt.savefig(plot_path)\n        plt.show()\n        plt.close()\n\n    # TEMPORAL SPLIT ANALYSIS (if temporal models are enabled)\n    if include_temporal:\n        if results.get(\"feature_importance_temporal\") is not None:\n            # Feature Importance Plot (Temporal Split)\n            sorted_idx_temporal = np.argsort(-results[\"feature_importance_temporal\"])[:TOP_FEATURES]\n            feature_names_temporal = X_train_temp.columns[sorted_idx_temporal]\n            fig, ax = plt.subplots(figsize=(12, 8))\n            sns.barplot(\n                x=results[\"feature_importance_temporal\"][sorted_idx_temporal],\n                y=feature_names_temporal,\n                ax=ax\n            )\n            ax.set_title(f\"Feature Importance - {model_name} - Temporal Split\")\n            ax.set_xlabel(\"Importance Score\")\n            ax.set_ylabel(\"Features\")\n            plt.tight_layout()\n            plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_feature_importance_temporal.png\")\n            plt.savefig(plot_path)\n            plt.show()\n            plt.close()\n\n        if results.get(\"perm_importance_temporal\") is not None:\n            # Permutation Importance Plot (Temporal Split)\n            perm_sorted_idx_temporal = np.argsort(-results[\"perm_importance_temporal\"])[:TOP_FEATURES]\n            feature_names_perm_temporal = X_train_temp.columns[perm_sorted_idx_temporal]\n            fig, ax = plt.subplots(figsize=(12, 8))\n            sns.barplot(\n                x=results[\"perm_importance_temporal\"][perm_sorted_idx_temporal],\n                y=feature_names_perm_temporal,\n                ax=ax\n            )\n            ax.set_title(f\"Permutation Importance - {model_name} - Temporal Split\")\n            ax.set_xlabel(\"Importance Score\")\n            ax.set_ylabel(\"Features\")\n            plt.tight_layout()\n            plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_permutation_importance_temporal.png\")\n            plt.savefig(plot_path)\n            plt.show()\n            plt.close()\n\n        if results.get(\"shap_values_temporal\") is not None:\n            # SHAP Summary Plot (Temporal Split)\n            fig, ax = plt.subplots(figsize=(12, 8))\n            shap.summary_plot(\n                results[\"shap_values_temporal\"], X_test_temp, max_display=TOP_FEATURES, show=False\n            )\n            plt.title(f\"SHAP Summary - {model_name} - Temporal Split\")\n            plot_path = os.path.join(feature_analysis_dir, f\"{model_name}_shap_temporal.png\")\n            plt.savefig(plot_path)\n            plt.show()\n            plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.501171Z","iopub.status.idle":"2025-04-20T13:37:21.501403Z","shell.execute_reply.started":"2025-04-20T13:37:21.501290Z","shell.execute_reply":"2025-04-20T13:37:21.501301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare a list to hold feature importance data for all models\nfeature_analysis_data = []\n\n# Loop through each model and extract feature importance results\nfor model_name, results in feature_analysis_results.items():\n    # Random Split Feature Importance\n    for i, feature_name in enumerate(X_train_rand.columns):\n        feature_analysis_data.append({\n            \"Model\": model_name,\n            \"Split\": \"Random\",\n            \"Feature\": feature_name,\n            \"Importance\": results[\"feature_importance_random\"][i]\n        })\n    \n    # Temporal Split Feature Importance\n    for i, feature_name in enumerate(X_train_temp.columns):\n        feature_analysis_data.append({\n            \"Model\": model_name,\n            \"Split\": \"Temporal\",\n            \"Feature\": feature_name,\n            \"Importance\": results[\"feature_importance_temporal\"][i]\n        })\n\n# Convert the list of dictionaries to a DataFrame\nfeature_analysis_df = pd.DataFrame(feature_analysis_data)\nfeature_analysis_df.sort_values(by=\"Importance\", ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.502394Z","iopub.status.idle":"2025-04-20T13:37:21.502683Z","shell.execute_reply.started":"2025-04-20T13:37:21.502538Z","shell.execute_reply":"2025-04-20T13:37:21.502566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random-Temporal Importance Comparison","metadata":{}},{"cell_type":"code","source":"# Calculate average importance per feature for each split\nrandom_split_importance = feature_analysis_df[feature_analysis_df['Split'] == 'Random'].groupby('Feature')['Importance'].mean()\ntemporal_split_importance = feature_analysis_df[feature_analysis_df['Split'] == 'Temporal'].groupby('Feature')['Importance'].mean()\n\n# Combine into a single DataFrame for comparison\nimportance_comparison = pd.DataFrame({\n    'Feature': random_split_importance.index,\n    'Random Importance': random_split_importance.values,\n    'Temporal Importance': temporal_split_importance.reindex(random_split_importance.index).values\n})\n\n# Calculate the difference and rank features by the difference\nimportance_comparison['Difference'] = importance_comparison['Random Importance'] - importance_comparison['Temporal Importance']\nimportance_comparison['Absolute Difference'] = importance_comparison['Difference'].abs()\n\n# Sort by absolute difference for analysis\nimportance_comparison = importance_comparison.sort_values(by='Absolute Difference', ascending=False)\nimportance_comparison.sort_values(by=\"Absolute Difference\", ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:37:21.504099Z","iopub.status.idle":"2025-04-20T13:37:21.504318Z","shell.execute_reply.started":"2025-04-20T13:37:21.504213Z","shell.execute_reply":"2025-04-20T13:37:21.504222Z"}},"outputs":[],"execution_count":null}]}